import json
import os
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import networkx as nx


class ConversationInsights:
    def __init__(self, analytics_engine=None):
        self.analytics_engine = analytics_engine
        self.conversation_flows = []
        self.user_journeys = defaultdict(list)
        self.topic_networks = {}
        self.response_quality_scores = {}
        
        print("[INSIGHTS] üîç Conversation Insights ƒë√£ kh·ªüi t·∫°o")

    def analyze_conversation_flow(self, user_id, days=30):
        """Ph√¢n t√≠ch lu·ªìng cu·ªôc tr√≤ chuy·ªán c·ªßa user"""
        if not self.analytics_engine:
            return None
        
        cutoff_date = datetime.now() - timedelta(days=days)
        
        # L·∫•y conversations c·ªßa user
        user_conversations = [
            conv for conv in self.analytics_engine.conversation_data
            if (conv['user_id'] == user_id and 
                datetime.fromisoformat(conv['timestamp']) > cutoff_date)
        ]
        
        if not user_conversations:
            return None
        
        # S·∫Øp x·∫øp theo th·ªùi gian
        user_conversations.sort(key=lambda x: x['timestamp'])
        
        # Ph√¢n t√≠ch patterns
        flow_analysis = {
            'total_conversations': len(user_conversations),
            'conversation_sessions': self.identify_sessions(user_conversations),
            'topic_transitions': self.analyze_topic_transitions(user_conversations),
            'sentiment_journey': self.analyze_sentiment_journey(user_conversations),
            'engagement_patterns': self.analyze_engagement_patterns(user_conversations),
            'response_satisfaction': self.analyze_response_satisfaction(user_conversations)
        }
        
        return flow_analysis

    def identify_sessions(self, conversations):
        """Nh·∫≠n di·ªán c√°c session chat ri√™ng bi·ªát"""
        sessions = []
        current_session = []
        session_gap_minutes = 30  # 30 ph√∫t kh√¥ng ho·∫°t ƒë·ªông = session m·ªõi
        
        for i, conv in enumerate(conversations):
            if i == 0:
                current_session = [conv]
            else:
                prev_time = datetime.fromisoformat(conversations[i-1]['timestamp'])
                curr_time = datetime.fromisoformat(conv['timestamp'])
                
                if (curr_time - prev_time).total_seconds() / 60 > session_gap_minutes:
                    # K·∫øt th√∫c session hi·ªán t·∫°i
                    sessions.append({
                        'session_id': len(sessions) + 1,
                        'start_time': current_session[0]['timestamp'],
                        'end_time': current_session[-1]['timestamp'],
                        'message_count': len(current_session),
                        'duration_minutes': (
                            datetime.fromisoformat(current_session[-1]['timestamp']) -
                            datetime.fromisoformat(current_session[0]['timestamp'])
                        ).total_seconds() / 60,
                        'topics': list(set(topic for conv in current_session for topic in conv['topics'])),
                        'sentiment_evolution': [conv['sentiment'] for conv in current_session]
                    })
                    
                    # B·∫Øt ƒë·∫ßu session m·ªõi
                    current_session = [conv]
                else:
                    current_session.append(conv)
        
        # Th√™m session cu·ªëi c√πng
        if current_session:
            sessions.append({
                'session_id': len(sessions) + 1,
                'start_time': current_session[0]['timestamp'],
                'end_time': current_session[-1]['timestamp'],
                'message_count': len(current_session),
                'duration_minutes': (
                    datetime.fromisoformat(current_session[-1]['timestamp']) -
                    datetime.fromisoformat(current_session[0]['timestamp'])
                ).total_seconds() / 60,
                'topics': list(set(topic for conv in current_session for topic in conv['topics'])),
                'sentiment_evolution': [conv['sentiment'] for conv in current_session]
            })
        
        return sessions

    def analyze_topic_transitions(self, conversations):
        """Ph√¢n t√≠ch chuy·ªÉn ƒë·ªïi ch·ªß ƒë·ªÅ"""
        transitions = []
        topic_sequences = []
        
        for conv in conversations:
            topic_sequences.extend(conv['topics'])
        
        # T·∫°o transition matrix
        transition_counts = defaultdict(lambda: defaultdict(int))
        
        for i in range(len(topic_sequences) - 1):
            current_topic = topic_sequences[i]
            next_topic = topic_sequences[i + 1]
            transition_counts[current_topic][next_topic] += 1
        
        # T√≠nh transition probabilities
        transition_probs = {}
        for from_topic, to_topics in transition_counts.items():
            total = sum(to_topics.values())
            transition_probs[from_topic] = {
                to_topic: count / total
                for to_topic, count in to_topics.items()
            }
        
        return {
            'transition_counts': dict(transition_counts),
            'transition_probabilities': transition_probs,
            'most_common_transitions': self.get_top_transitions(transition_counts),
            'topic_persistence': self.calculate_topic_persistence(topic_sequences)
        }

    def get_top_transitions(self, transition_counts, top_n=5):
        """L·∫•y top transitions ph·ªï bi·∫øn nh·∫•t"""
        all_transitions = []
        
        for from_topic, to_topics in transition_counts.items():
            for to_topic, count in to_topics.items():
                if from_topic != to_topic:  # Lo·∫°i b·ªè self-transitions
                    all_transitions.append((from_topic, to_topic, count))
        
        return sorted(all_transitions, key=lambda x: x[2], reverse=True)[:top_n]

    def calculate_topic_persistence(self, topic_sequences):
        """T√≠nh ƒë·ªô b·ªÅn v·ªØng c·ªßa ch·ªß ƒë·ªÅ"""
        if len(topic_sequences) < 2:
            return 0
        
        same_topic_count = sum(1 for i in range(len(topic_sequences) - 1) 
                              if topic_sequences[i] == topic_sequences[i + 1])
        
        return same_topic_count / (len(topic_sequences) - 1)

    def analyze_sentiment_journey(self, conversations):
        """Ph√¢n t√≠ch h√†nh tr√¨nh c·∫£m x√∫c"""
        sentiments = [conv['sentiment'] for conv in conversations]
        
        # T√≠nh sentiment transitions
        sentiment_transitions = defaultdict(lambda: defaultdict(int))
        for i in range(len(sentiments) - 1):
            current = sentiments[i]
            next_sentiment = sentiments[i + 1]
            sentiment_transitions[current][next_sentiment] += 1
        
        # T√≠nh sentiment stability
        stability_score = sum(1 for i in range(len(sentiments) - 1) 
                             if sentiments[i] == sentiments[i + 1]) / max(1, len(sentiments) - 1)
        
        # T√¨m sentiment patterns
        patterns = self.find_sentiment_patterns(sentiments)
        
        return {
            'sentiment_sequence': sentiments,
            'sentiment_transitions': dict(sentiment_transitions),
            'stability_score': stability_score,
            'dominant_sentiment': Counter(sentiments).most_common(1)[0] if sentiments else None,
            'sentiment_patterns': patterns,
            'sentiment_improvement': self.calculate_sentiment_trend(sentiments)
        }

    def find_sentiment_patterns(self, sentiments):
        """T√¨m patterns trong sentiment sequence"""
        patterns = []
        
        # Pattern: Negative -> Positive (recovery)
        for i in range(len(sentiments) - 1):
            if sentiments[i] == 'negative' and sentiments[i + 1] == 'positive':
                patterns.append(('recovery', i))
        
        # Pattern: Positive -> Negative (decline)
        for i in range(len(sentiments) - 1):
            if sentiments[i] == 'positive' and sentiments[i + 1] == 'negative':
                patterns.append(('decline', i))
        
        # Pattern: Consistent positive streak
        streak_count = 0
        for sentiment in sentiments:
            if sentiment == 'positive':
                streak_count += 1
            else:
                if streak_count >= 3:
                    patterns.append(('positive_streak', streak_count))
                streak_count = 0
        
        return patterns

    def calculate_sentiment_trend(self, sentiments):
        """T√≠nh xu h∆∞·ªõng c·∫£m x√∫c (c·∫£i thi·ªán/x·∫•u ƒëi)"""
        if len(sentiments) < 2:
            return 0
        
        # Chuy·ªÉn sentiment th√†nh s·ªë
        sentiment_values = {'negative': -1, 'neutral': 0, 'positive': 1}
        values = [sentiment_values[s] for s in sentiments]
        
        # T√≠nh trend b·∫±ng linear regression ƒë∆°n gi·∫£n
        n = len(values)
        x = list(range(n))
        
        # Slope c·ªßa regression line
        slope = (n * sum(x[i] * values[i] for i in range(n)) - sum(x) * sum(values)) / \
                (n * sum(x[i]**2 for i in range(n)) - sum(x)**2)
        
        return slope

    def analyze_engagement_patterns(self, conversations):
        """Ph√¢n t√≠ch patterns t∆∞∆°ng t√°c"""
        message_lengths = [conv['message_length'] for conv in conversations]
        response_times = [conv['response_time'] for conv in conversations]
        
        # T√≠nh engagement metrics
        avg_message_length = np.mean(message_lengths)
        message_length_trend = self.calculate_trend(message_lengths)
        response_time_trend = self.calculate_trend(response_times)
        
        # Ph√¢n t√≠ch message frequency
        timestamps = [datetime.fromisoformat(conv['timestamp']) for conv in conversations]
        time_gaps = [(timestamps[i+1] - timestamps[i]).total_seconds() 
                    for i in range(len(timestamps) - 1)]
        
        return {
            'avg_message_length': avg_message_length,
            'message_length_trend': message_length_trend,
            'response_time_trend': response_time_trend,
            'avg_time_between_messages': np.mean(time_gaps) if time_gaps else 0,
            'engagement_consistency': 1 / (1 + np.std(message_lengths)) if message_lengths else 0,
            'conversation_intensity': len(conversations) / max(1, (timestamps[-1] - timestamps[0]).days)
        }

    def calculate_trend(self, values):
        """T√≠nh trend c·ªßa m·ªôt chu·ªói gi√° tr·ªã"""
        if len(values) < 2:
            return 0
        
        n = len(values)
        x = list(range(n))
        
        slope = (n * sum(x[i] * values[i] for i in range(n)) - sum(x) * sum(values)) / \
                (n * sum(x[i]**2 for i in range(n)) - sum(x)**2)
        
        return slope

    def analyze_response_satisfaction(self, conversations):
        """Ph√¢n t√≠ch m·ª©c ƒë·ªô h√†i l√≤ng v·ªõi response"""
        satisfaction_indicators = []
        
        for i, conv in enumerate(conversations):
            # Indicators c·ªßa satisfaction
            user_msg = conv['user_message'].lower()
            bot_response = conv['bot_response'].lower()
            
            # Positive indicators
            positive_words = ['thanks', 'thank you', 'good', 'great', 'awesome', 'perfect', 
                            'c·∫£m ∆°n', 't·ªët', 'hay', 'ƒë∆∞·ª£c', 'ok', 'oke']
            
            # Negative indicators  
            negative_words = ['wrong', 'bad', 'not good', 'terrible', 'sai', 't·ªá', 'kh√¥ng t·ªët', 
                            'd·ªü', 'kh√¥ng ƒë√∫ng', 'ch·∫≥ng hi·ªÉu']
            
            # Confusion indicators
            confusion_words = ['what', 'huh', 'confused', 'kh√¥ng hi·ªÉu', 'g√¨', 'sao', 'h·∫£']
            
            satisfaction_score = 0
            
            # Ki·ªÉm tra next message c·ªßa user (n·∫øu c√≥)
            if i < len(conversations) - 1:
                next_msg = conversations[i + 1]['user_message'].lower()
                
                if any(word in next_msg for word in positive_words):
                    satisfaction_score += 1
                elif any(word in next_msg for word in negative_words):
                    satisfaction_score -= 1
                elif any(word in next_msg for word in confusion_words):
                    satisfaction_score -= 0.5
            
            # Ki·ªÉm tra response time (nhanh = t·ªët)
            if conv['response_time'] < 2:
                satisfaction_score += 0.3
            elif conv['response_time'] > 10:
                satisfaction_score -= 0.3
            
            # Ki·ªÉm tra response length (qu√° ng·∫Øn ho·∫∑c qu√° d√†i c√≥ th·ªÉ kh√¥ng t·ªët)
            response_length = len(bot_response)
            if 20 <= response_length <= 200:
                satisfaction_score += 0.2
            elif response_length < 5 or response_length > 500:
                satisfaction_score -= 0.2
            
            satisfaction_indicators.append(satisfaction_score)
        
        return {
            'satisfaction_scores': satisfaction_indicators,
            'avg_satisfaction': np.mean(satisfaction_indicators) if satisfaction_indicators else 0,
            'satisfaction_trend': self.calculate_trend(satisfaction_indicators),
            'highly_satisfied_count': sum(1 for score in satisfaction_indicators if score > 0.5),
            'dissatisfied_count': sum(1 for score in satisfaction_indicators if score < -0.5)
        }

    def cluster_conversation_topics(self, conversations, n_clusters=5):
        """Gom c·ª•m conversations theo ch·ªß ƒë·ªÅ"""
        if len(conversations) < n_clusters:
            return None
        
        # T·∫°o text corpus
        texts = [conv['user_message'] for conv in conversations]
        
        # TF-IDF vectorization
        vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
        try:
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # K-means clustering
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)
            
            # Ph√¢n t√≠ch clusters
            clusters = defaultdict(list)
            for i, label in enumerate(cluster_labels):
                clusters[label].append(conversations[i])
            
            # T·∫°o cluster summaries
            cluster_summaries = {}
            for cluster_id, cluster_convs in clusters.items():
                topics = [topic for conv in cluster_convs for topic in conv['topics']]
                common_topics = Counter(topics).most_common(3)
                
                cluster_summaries[cluster_id] = {
                    'size': len(cluster_convs),
                    'common_topics': common_topics,
                    'avg_sentiment': self.calculate_avg_sentiment(cluster_convs),
                    'sample_messages': [conv['user_message'] for conv in cluster_convs[:3]]
                }
            
            return cluster_summaries
            
        except Exception as e:
            print(f"[INSIGHTS] ‚ö†Ô∏è L·ªói clustering: {str(e)}")
            return None

    def calculate_avg_sentiment(self, conversations):
        """T√≠nh sentiment trung b√¨nh c·ªßa m·ªôt nh√≥m conversations"""
        sentiment_values = {'negative': -1, 'neutral': 0, 'positive': 1}
        values = [sentiment_values[conv['sentiment']] for conv in conversations]
        
        avg_value = np.mean(values)
        
        if avg_value > 0.3:
            return 'positive'
        elif avg_value < -0.3:
            return 'negative'
        else:
            return 'neutral'

    def build_topic_network(self, conversations):
        """X√¢y d·ª±ng m·∫°ng l∆∞·ªõi ch·ªß ƒë·ªÅ"""
        G = nx.Graph()
        
        # Th√™m edges gi·ªØa c√°c topics xu·∫•t hi·ªán c√πng nhau
        for conv in conversations:
            topics = conv['topics']
            for i in range(len(topics)):
                for j in range(i + 1, len(topics)):
                    if G.has_edge(topics[i], topics[j]):
                        G[topics[i]][topics[j]]['weight'] += 1
                    else:
                        G.add_edge(topics[i], topics[j], weight=1)
        
        # T√≠nh centrality measures
        centrality = nx.degree_centrality(G)
        betweenness = nx.betweenness_centrality(G)
        
        return {
            'graph': G,
            'centrality': centrality,
            'betweenness': betweenness,
            'most_central_topics': sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:5],
            'bridge_topics': sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:5]
        }

    def generate_insights_report(self, user_id, days=30):
        """T·∫°o b√°o c√°o insights t·ªïng h·ª£p"""
        flow_analysis = self.analyze_conversation_flow(user_id, days)
        
        if not flow_analysis:
            return None
        
        # L·∫•y th√™m d·ªØ li·ªáu t·ª´ analytics engine
        user_insights = self.analytics_engine.get_user_insights(user_id) if self.analytics_engine else {}
        
        report = {
            'user_id': user_id,
            'analysis_period_days': days,
            'generated_at': datetime.now().isoformat(),
            'conversation_flow': flow_analysis,
            'user_profile': user_insights,
            'key_insights': self.extract_key_insights(flow_analysis, user_insights),
            'recommendations': self.generate_recommendations(flow_analysis, user_insights)
        }
        
        return report

    def extract_key_insights(self, flow_analysis, user_insights):
        """Tr√≠ch xu·∫•t insights ch√≠nh"""
        insights = []
        
        # Session insights
        sessions = flow_analysis.get('conversation_sessions', [])
        if sessions:
            avg_session_duration = np.mean([s['duration_minutes'] for s in sessions])
            insights.append(f"User c√≥ {len(sessions)} sessions v·ªõi th·ªùi gian trung b√¨nh {avg_session_duration:.1f} ph√∫t")
        
        # Sentiment insights
        sentiment_journey = flow_analysis.get('sentiment_journey', {})
        if sentiment_journey:
            trend = sentiment_journey.get('sentiment_improvement', 0)
            if trend > 0.1:
                insights.append("C·∫£m x√∫c c·ªßa user c√≥ xu h∆∞·ªõng t√≠ch c·ª±c")
            elif trend < -0.1:
                insights.append("C·∫£m x√∫c c·ªßa user c√≥ xu h∆∞·ªõng ti√™u c·ª±c")
        
        # Engagement insights
        engagement = flow_analysis.get('engagement_patterns', {})
        if engagement:
            if engagement.get('message_length_trend', 0) > 0:
                insights.append("User ng√†y c√†ng vi·∫øt tin nh·∫Øn d√†i h∆°n (tƒÉng engagement)")
            
            intensity = engagement.get('conversation_intensity', 0)
            if intensity > 5:
                insights.append("User r·∫•t t√≠ch c·ª±c t∆∞∆°ng t√°c (>5 conversations/ng√†y)")
        
        # Topic insights
        topic_transitions = flow_analysis.get('topic_transitions', {})
        if topic_transitions:
            persistence = topic_transitions.get('topic_persistence', 0)
            if persistence > 0.7:
                insights.append("User c√≥ xu h∆∞·ªõng t·∫≠p trung v√†o m·ªôt ch·ªß ƒë·ªÅ")
            elif persistence < 0.3:
                insights.append("User th∆∞·ªùng chuy·ªÉn ƒë·ªïi ch·ªß ƒë·ªÅ")
        
        return insights

    def generate_recommendations(self, flow_analysis, user_insights):
        """T·∫°o recommendations d·ª±a tr√™n insights"""
        recommendations = []
        
        # Response satisfaction recommendations
        satisfaction = flow_analysis.get('response_satisfaction', {})
        if satisfaction:
            avg_satisfaction = satisfaction.get('avg_satisfaction', 0)
            if avg_satisfaction < 0:
                recommendations.append("C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng response - user c√≥ v·∫ª kh√¥ng h√†i l√≤ng")
            
            dissatisfied_count = satisfaction.get('dissatisfied_count', 0)
            if dissatisfied_count > 3:
                recommendations.append("C·∫ßn xem x√©t l·∫°i prompt v√† response generation")
        
        # Engagement recommendations
        engagement = flow_analysis.get('engagement_patterns', {})
        if engagement:
            if engagement.get('response_time_trend', 0) > 0:
                recommendations.append("Response time ƒëang tƒÉng - c·∫ßn t·ªëi ∆∞u performance")
            
            consistency = engagement.get('engagement_consistency', 0)
            if consistency < 0.5:
                recommendations.append("TƒÉng consistency trong response ƒë·ªÉ gi·ªØ engagement")
        
        # Topic recommendations
        if user_insights:
            favorite_topic = user_insights.get('favorite_topic')
            if favorite_topic and favorite_topic != 'general':
                recommendations.append(f"User quan t√¢m ƒë·∫øn {favorite_topic} - c√≥ th·ªÉ personalize responses")
        
        return recommendations

    def save_insights_report(self, user_id, filename=None):
        """L∆∞u b√°o c√°o insights"""
        if not filename:
            filename = f"analytics/insights_{user_id}_{datetime.now().strftime('%Y%m%d')}.json"
        
        try:
            report = self.generate_insights_report(user_id)
            if report:
                os.makedirs(os.path.dirname(filename), exist_ok=True)
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
                
                print(f"[INSIGHTS] üíæ Insights report saved: {filename}")
                return True
            else:
                print(f"[INSIGHTS] ‚ö†Ô∏è No data available for user {user_id}")
                return False
        except Exception as e:
            print(f"[INSIGHTS] ‚ùå Error saving report: {str(e)}")
            return False


# Singleton instance
_conversation_insights = None

def get_conversation_insights(analytics_engine=None):
    """L·∫•y instance singleton c·ªßa ConversationInsights"""
    global _conversation_insights
    if _conversation_insights is None:
        _conversation_insights = ConversationInsights(analytics_engine)
    return _conversation_insights


if __name__ == "__main__":
    # Test conversation insights
    from analytics_engine import get_analytics_engine
    
    analytics = get_analytics_engine()
    insights = get_conversation_insights(analytics)
    
    # Add some test data
    analytics.record_conversation("Hello", "Hi there!", "test_user", 1.0)
    analytics.record_conversation("How are you?", "I'm good, thanks!", "test_user", 1.5)
    analytics.record_conversation("What's the weather?", "I don't know weather info", "test_user", 2.0)
    
    # Generate insights report
    report = insights.generate_insights_report("test_user", days=1)
    if report:
        print("Insights report generated successfully!")
        print(f"Key insights: {report['key_insights']}")
        print(f"Recommendations: {report['recommendations']}")
    else:
        print("No insights available") 